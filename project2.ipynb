{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d01733f0",
   "metadata": {},
   "source": [
    "## Continuación del proyecto - Parte 2 <p> Implementar una búsqueda TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26b607f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ae9af",
   "metadata": {},
   "source": [
    "### Contruimos el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0461419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all tokenized texts into a single list\n",
    "tokenized_texts = [doc[\"tokenized_text\"] for doc in data]\n",
    "\n",
    "# flatten the list of lists\n",
    "vocab = list(itertools.chain(*tokenized_texts))\n",
    "\n",
    "# remove duplicates\n",
    "vocab = list(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dabd3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF SE CALCULA COMO:\n",
    "\n",
    "# TF = frecuencia del término en el documento (B) / longitud del documento\n",
    "# IDF = log ( nº documentos / nº docs en los que aparece el término (C) )\n",
    "\n",
    "# Entonces TFIDF = TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03c9c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos:\n",
    "# Calcular cuántas veces aparece cada palabra en cada documento - B\n",
    "# Calcular en cuántos documentos aparece cada palabra - C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5236036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times each token occurs in a document - B\n",
    "docs_token_counter = []\n",
    "for doc in data:\n",
    "    doc_tokenized = doc[\"tokenized_text\"]\n",
    "    docs_token_counter.append(Counter(doc_tokenized)) # Counter crea un diccionario para cada documento de la forma:\n",
    "                                                      # {palabra: nº apariciones en el documento}\n",
    "\n",
    "\n",
    "#for count, value in enumerate(docs_token_counter):\n",
    "#    print(count, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cd96d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each token in corpus vocabulary, count in how many documents it occurs - C\n",
    "number_docs_with_token  = {}\n",
    "for token in vocab:\n",
    "    count_docs = sum([1 for doc in docs_token_counter if token in doc.keys()])\n",
    "    number_docs_with_token[token] = count_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc09c1a",
   "metadata": {},
   "source": [
    "### Calcular TFIDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1466b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(docs_token_counter): # asigna un ordinal a cada diccionario (que corresponde a un documento)\n",
    "    doc_length = len(doc)\n",
    "    tfidf_vec = []\n",
    "    for token in vocab:\n",
    "        \n",
    "        # compute a term frequency (tf) per document\n",
    "        tf = doc[token] / len(data[i][\"tokenized_text\"])\n",
    "        \n",
    "        # compute a log of inverse document frequency per document\n",
    "\n",
    "        idf = np.log(len(data)/number_docs_with_token[token])\n",
    "\n",
    "        tfidf = tf * idf\n",
    "        tfidf_vec.append(tfidf)\n",
    "    \n",
    "    # add tf_idf vector to the dictionaries\n",
    "    data[i]['tf_idf'] = tfidf_vec\n",
    "    \n",
    "# Save an updates summary with computed Tf-Idf vectors\n",
    "with open('tfidf.json', 'w') as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "818f8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Mi intento\n",
    "\n",
    "def tfidf(vocab, tokenized_text): # diccionario, lista\n",
    "    # calculamos el vector tfidf de cada documento\n",
    "    tfidf = dict.fromkeys(vocabulary.word2count, 0)\n",
    "    for token in tokenized_text:\n",
    "        tf = tokenized_text.count(token) # fecuencia del término en el documento dado\n",
    "        idf = 1/(1 + vocabulary.word2count[token]) # 1/(1 + frecuencia del término en la colección)\n",
    "        tfidf[token] = tf*idf # los tokens no presentes en el documento actual se quedan con valor tfidf 0\n",
    "    return tfidf\n",
    "    \n",
    "with open('tokenized.json', 'r') as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "    for doc in data:\n",
    "        doc['tfidf'] = tfidf(vocabulario, doc['tokenized_text']) # Nuevo campo con los vectores tfidf\n",
    "        \n",
    "with open('tfidf.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c98e4",
   "metadata": {},
   "source": [
    "### compute cosine similarities between the document Tf-Idf vectors and the query Tf-Idf vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b175e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"which is the worst illness?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f03a8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We suggest you use scikit-learn library and its cosine_similarity function\n",
    "\n",
    "# reutilizamos la función process para tokenizar la consulta\n",
    "\n",
    "# Reimplementamos el calculo de vector TFIDF para la consulta\n",
    "\n",
    "def vectorize(query, vocab = vocab):\n",
    "    \n",
    "    query_tokenized = process(query)\n",
    "    query_token_counter = Counter(query_tokenized)\n",
    "    query_vec = []\n",
    "    for token in vocab:\n",
    "        \n",
    "        tf = query_token_counter[token] / len(query_tokenized)\n",
    "        idf = np.log(len(data) /  number_docs_with_token[token])\n",
    "        tfidf = tf * idf\n",
    "        query_vec.append(tfidf)\n",
    "            \n",
    "    return query_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "801342af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmi intento:\\ndef search(document_tfidf, query): # document_tfidf es el vector tfidf del documento, query es una consulta de tipo cadena\\n    query_tfidf = list(tfidf(vocabulario, list(query.split())).values())\\n    document_ = list(document_tfidf.values())\\n    sklearn.metrics.pairwise.cosine_similarity(query_tfidf.reshape(-1, 1), document_.reshape(-1, 1))\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#función de búsqueda\n",
    "\n",
    "def search_tfidf(query, docs):\n",
    "    \n",
    "    # vectorize query\n",
    "    query_vec = vectorize(query)\n",
    "    query_arr = np.array(query_vec)\n",
    "    \n",
    "    # Build a list of results and their cosine similarity scores\n",
    "    rankings = []\n",
    "    for doc in docs:\n",
    "        doc_rank = {}\n",
    "        doc_arr = np.array(doc['tf_idf'])\n",
    "        rank = cosine_similarity(query_arr.reshape(1,-1), doc_arr.reshape(1, -1))[0][0]\n",
    "        if rank > 0:\n",
    "            doc_rank['title'] = doc['title']\n",
    "            doc_rank['rank'] = rank\n",
    "            rankings.append(doc_rank)\n",
    "\n",
    "    #return sorted results\n",
    "    return sorted(rankings, key=lambda k: k['rank'], reverse=True)\n",
    "ranking = search_tfidf(query, data)\n",
    "\n",
    "\n",
    "'''\n",
    "mi intento: da problemas con las palabras de la consulta que NO están en el vocabulario\n",
    "def search(document_tfidf, query): # document_tfidf es el vector tfidf del documento, query es una consulta de tipo cadena\n",
    "    query_tfidf = list(tfidf(vocabulario, list(query.split())).values())\n",
    "    document_ = list(document_tfidf.values())\n",
    "    sklearn.metrics.pairwise.cosine_similarity(query_tfidf.reshape(-1, 1), document_.reshape(-1, 1))\n",
    "    \n",
    "similarities = []\n",
    "with open('tfidf.json', 'r') as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "    for doc in data:\n",
    "        similarities.append(search(doc['tfidf'], 'illness'))\n",
    "print(similarities)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e9eced1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Spanish flu', 'rank': 0.09304508055751907},\n",
       " {'title': 'HIV/AIDS', 'rank': 0.08528298155751908},\n",
       " {'title': 'Swine influenza', 'rank': 0.079167190098216},\n",
       " {'title': 'Superspreader', 'rank': 0.07005576161414198}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tfidf(\"which is the worst illness\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f823d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
